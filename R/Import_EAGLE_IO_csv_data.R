#' Imports sonde, height, and analyte data from Eagle IO
#'
#' Imports data from csv files generated by Eagle IO export. Users could change how data is imported from Eagle IO but should ensure outputs follow structure in rds file generated by internal functions below
#'
#' @param user_data_folder file path to user data folder
#' @param site site folder in user_data_folder
#' @note
#' All functions expect data to be in following folders:
#'
#' \describe{
#'   \item{1. user_data_folder/site/ADCP_data}{- Place 000 files here}
#'    }
#' \describe{
#'   \item{2. user_data_folder/site/Analyte_data}{- Place TSS data from EAGLE IO csv export here}
#'    }
#'    \describe{
#'   \item{3. user_data_folder/site/Channel_Geometry}{- Place cross-section survety in xs.RDS file format from Stephen Wallace here}
#'    }
#'    \describe{
#'   \item{4. user_data_folder/site/Discharge_data }{- Place discharge and velocity csv files from Stephen Wallace here}
#'    }
#'    \describe{
#'   \item{5. user_data_folder/site/Height_Offsets}{- Place height offset data from EAGLE IO csv export here}
#'    }
#'    \describe{
#'   \item{6. user_data_folder/site/Sonde_and_Height_data}{- Place TSS data from EAGLE IO csv export here}
#'    }
#'
#' Where "site" is a specific site (e.g., for Johnstone at Innisfail use site <- "JRI")
#'
#' See vignette('LinkrealTimeloads') for further instructions
#'
#' @section Warning:
#' Users could change how data is imported from Eagle IO but should ensure outputs follow structure in rds file generated by internal functions below
#'
#' @return rds files with sonde, height, and analyte data used in Link_to_Real_time_loads() and Predict_TSS_from_SCI(). Outputs written to site/Sonde_and_Height_data/Sonde.rds, site/Sonde_and_Height_data/Height.rds, and site/Analyte_data/Sediment_Samples.rds
#' @seealso
#' \code{\link{Link_to_Real_time_loads}} Process data in specified folder structure using realTimeloads package
#' \code{\link{Predict_TSS_from_SCI}} Estimates TSS from processed acoustic backscatter and turbidit
#'
#' @author Daniel Livsey (2023) ORCID: 0000-0002-2028-6128
#' @examples
#' # See vignette('LinkrealTimeloads',package = LinkrealTimeloads)
#' @references
#' Stephen Wallace (2023, DES) provided crucial functions in Import_Channelmaster_Data() to extract data from binary 000 files.
#'
#' Livsey D.N. (2023). realTimeloads: Analyte Flux and Load from Estimates of Concentration and Discharge_. R package version 1.0.0.
#'
#' Livsey, D.N. (in review). National Industry Guidelines for hydrometric monitoringâ€“Part 12: Application of acoustic Doppler velocity meters to measure suspended-sediment load. Bureau of Meteorology. Melbourne, Australia
#'
#' @export
Import_EAGLE_IO_csv_data <- function(user_data_folder,site = NULL) {
# user_data_folder: file path to folder containing all site folders
# site: specific site folder name (e.g., "MRD" for Mulgrave River at Deeral)
# Imports TSS data, datum offsets, and timeseries of height and sonde data from Eagle IO
# reads in data and updates new data to rds file if present
# imports analyte, height-offsets, sonde, and height data
# Daniel Livsey, 2023

Import_eagle_io_analyte_data_csv(user_data_folder,site)
Import_eagle_io_height_offset_data_csv(user_data_folder,site)
Import_eagle_io_timeseries_csv(user_data_folder,site)

msg <- "EAGLE IO CSV FILE DATA IMPORTED"
return(msg)
}

# Read-in TSS data
Import_eagle_io_analyte_data_csv <- function(user_data_folder,site = NULL) {
  # user_data_folder: file path to folder containing all site folders
  # site: specific site folder name (e.g., "MRD" for Mulgrave River at Deeral)
  # reads in data and updates new data to rds file if present
  # Daniel Livsey, 2023

  #site <- "MRD"
  #user_data_folder <- "C:/Users/livseyd/OneDrive - Queensland University of Technology/Documents/R/LoadDashboard/user_data"

  all_site_folders <- list.dirs(user_data_folder,recursive = "FALSE")

  # get specific site folder if site is provided
  if (!is.null(site)) {
    all_site_folders <- all_site_folders[grepl(site,all_site_folders)]
  }

  s <- strsplit(all_site_folders,'/')

  number_of_sites <- length(all_site_folders)

  for (k in 1:number_of_sites) {
    site <- s[[k]][length(s[[k]])]


    Site <-readRDS(paste0(user_data_folder,'/Site_List.rds'))
    Site_Number <- Site$Site_Number[is.element(Site$Site_Folder,site)]


    # folder paths
    site_folder <- paste0(paste0(user_data_folder,"/"),site)
    Analyte_folder <- paste0(site_folder,"/Analyte_data")

    file <- list.files(path = Analyte_folder, pattern = "*\\.csv", recursive = TRUE,full.names = TRUE,include.dirs = TRUE)

    # at JRI we use JRC data at times
    if (grepl('1120054',file)) {
      Site_Number <-  '1120054'
    }


    if (length(file)>0) {
      # Import csv
      d<-read.csv(file)
      vars <- d[1,]
      #print(vars)
      iTSS <- grepl("TSS",vars)

      colnames(d)[1] <- "time"
      colnames(d)[iTSS] <- "SSCpt_mg_per_liter"
      d$Site_number <- Site_Number


      Data <- d[3:nrow(d),]
      time <- as.POSIXct(Data$time,format = "%Y-%m-%d %H:%M:%S",tz = "Australia/Brisbane")
      SSCpt_mg_per_liter <- as.numeric(Data$SSCpt_mg_per_liter)
      Site_number <- Data$Site_number

      Sediment_Samples = data.frame(time,SSCpt_mg_per_liter,Site_number)
      # only retain rows with data
      ind <- !is.na(SSCpt_mg_per_liter)
      Sediment_Samples <- Sediment_Samples[ind,]

      fileRDS <- paste0(Analyte_folder,"/Sediment_Samples.rds")
      # Check if any new data was imported if rds file is already present
      if (file.exists(fileRDS)) {
        Sedo <- readRDS(fileRDS)
        inew <- !is.element(Sedo$time,Sediment_Samples$time)
        Output <- rbind(Sediment_Samples,Sedo[inew,]) # combine
        Output <- Output[order(Output$time),] # order time
        Output <- Output[!duplicated(Output[,c('time')]),] # remove duplicate times
        Sediment_Samples <- Output
      }

      # save to RDS
      saveRDS(Sediment_Samples,fileRDS)

    }
  }
  msg <- ".RDS FILE(S) SAVED IN: SITE/Analyte_data"
  return(NULL)

}

# Read-in ADCP, Intake, and Cap Level (i.e, capillary line level used for height) datum offsets
Import_eagle_io_height_offset_data_csv <- function(user_data_folder,site = NULL) {
  # user_data_folder: file path to folder containing all site folders
  # site: specific site folder name (e.g., "MRD" for Mulgrave River at Deeral)
  # reads in data and updates new data to rds file if present
  # Daniel Livsey, 2023

  #site <- "JRI"
  #user_data_folder <- "C:/Users/livseyd/OneDrive - Queensland University of Technology/Documents/R/LoadDashboard/user_data"

  all_site_folders <- list.dirs(user_data_folder,recursive = "FALSE")

  # get specific site folder if site is provided
  if (!is.null(site)) {
    all_site_folders <- all_site_folders[grepl(site,all_site_folders)]
  }

  s <- strsplit(all_site_folders,'/')

  number_of_sites <- length(all_site_folders)

  for (k in 1:number_of_sites) {
    site <- s[[k]][length(s[[k]])]

    # folder paths
    site_folder <- paste0(paste0(user_data_folder,"/"),site)
    offset_folder <- paste0(site_folder,"/Height_Offsets")

    Site <-readRDS(paste0(user_data_folder,'/Site_List.rds'))
    Site_Number <- Site$Site_Number[is.element(Site$Site_Folder,site)]


    file <- list.files(path = offset_folder, pattern = "*\\.csv", recursive = TRUE,full.names = TRUE,include.dirs = TRUE)
    if (length(file)>0) {
      d<-read.csv(file)
      vars <- d[1,]
      #print(vars)
      iADCP <- grepl("ADCPLevelOffset",vars)
      iIntake <- grepl("Intake",vars)
      iPressure <- grepl("CapLevel",vars) # elevation of pressure line

      colnames(d)[1] <- "time"
      colnames(d)[iADCP] <- "ADCP_elevation_above_gauge_datum_m"
      colnames(d)[iIntake] <- "Intake_elevation_above_gauge_datum_m"
      colnames(d)[iPressure] <- "Pressure_line_elevation_above_gauge_datum_m"
      d$Site_number <- Site_Number


      Data <- d[3:nrow(d),]
      time <- as.POSIXct(Data$time,format = "%Y-%m-%d %H:%M:%S",tz = "Australia/Brisbane")
      ADCP_elevation_above_gauge_datum_m <- as.numeric(Data$ADCP_elevation_above_gauge_datum_m)

      Intake_elevation_above_gauge_datum_m <- as.numeric(Data$Intake_elevation_above_gauge_datum_m)

      Pressure_line_elevation_above_gauge_datum_m <- as.numeric(Data$Pressure_line_elevation_above_gauge_datum_m)

      Site_number <- Data$Site_number

      if (sum(iPressure)==1) {
        Offsets = data.frame(time,ADCP_elevation_above_gauge_datum_m,Pressure_line_elevation_above_gauge_datum_m,Intake_elevation_above_gauge_datum_m,Site_number)
      }

      if (sum(iPressure)==0) {
        Offsets = data.frame(time,ADCP_elevation_above_gauge_datum_m,Intake_elevation_above_gauge_datum_m,Site_number)
      }

      fileRDS <- paste0(offset_folder,"/Height_Offsets.rds")
      # Check if any new data was imported if rds file is already present
      if (file.exists(fileRDS)) {
        Ho <- readRDS(fileRDS)
        inew <- !is.element(Ho$time,Offsets$time)
        Output <- rbind(Offsets,Ho[inew,]) # combine
        Output <- Output[order(Output$time),] # order time
        Output <- Output[!duplicated(Output[,c('time')]),] # remove duplicate times
        Offsets <- Output
      }

      #print(site)
      #print(fileRDS)
      # save to RDS
      saveRDS(Offsets,fileRDS)
    }

  }


  msg <- ".RDS FILE(S) SAVED IN: SITE/Height_Offsets"

  return(NULL)

}

# Read-in height and sonde timeseries
Import_eagle_io_timeseries_csv <- function(user_data_folder,site = NULL) {
  # user_data_folder: file path to folder containing all site folders
  # site: specific site folder name (e.g., "MRD" for Mulgrave River at Deeral)
  # reads in data and updates new data to rds file if present
  # Daniel Livsey, 2023

  #site <- "MRD"
  #user_data_folder <- "C:/Users/livseyd/OneDrive - Queensland University of Technology/Documents/R/LoadDashboard/user_data"

  all_site_folders <- list.dirs(user_data_folder,recursive = "FALSE")

  # get specific site folder if site is provided
  if (!is.null(site)) {
    all_site_folders <- all_site_folders[grepl(site,all_site_folders)]
  }

  s <- strsplit(all_site_folders,'/')

  number_of_sites <- length(all_site_folders)

  for (k in 1:number_of_sites) {
    site <- s[[k]][length(s[[k]])]

    # folder paths
    site_folder <- paste0(paste0(user_data_folder,"/"),site)
    Sonde_and_Height_folder <- paste0(site_folder,"/Sonde_and_Height_data")

    Site <-readRDS(paste0(user_data_folder,'/Site_List.rds'))
    Site_Number <- Site$Site_Number[is.element(Site$Site_Folder,site)]

    file <- list.files(path = Sonde_and_Height_folder, pattern = "*\\.csv", recursive = TRUE,full.names = TRUE,include.dirs = TRUE)

    fileRDS <- c(paste0(Sonde_and_Height_folder,'/Height.rds'),paste0(Sonde_and_Height_folder,'/Sonde.rds'))

    if (length(file)>0) {
      for (i in 1:length(file)) { # at JRI also import sonde data from JRC
      # read-in Eagle IO csv file
      d<-read.csv(file[i])
      vars <- d[1,]
      #print(vars)
      iH <- grepl("Level",vars)
      iCond <- grepl("Sonde - Conductivity",vars)
      iTemp <- grepl("Sonde - SondeTemp",vars)
      iTurb <- grepl("Sonde - Turbidity",vars)

      colnames(d)[1] <- "time"
      colnames(d)[iH] <- "Height_m"
      colnames(d)[iTemp] <- "Water_Temperature_degC"
      colnames(d)[iCond] <- "Conductivity_uS_per_cm"
      colnames(d)[iTurb] <- "Turbidity_FNU"

      # at JRI we use JRC data at times
      if (grepl('1120054',file[i])) {
      Site_Number <-  '1120054'
      }
      d$Site_number <- Site_Number

      Data <- d[3:nrow(d),]
      time <- as.POSIXct(Data$time,format = "%Y-%m-%d %H:%M:%S",tz = "Australia/Brisbane")
      Height_m <- as.numeric(Data$Height_m)

      Water_Temperature_degC <- as.numeric(Data$Water_Temperature_degC)
      Conductivity_uS_per_cm <- as.numeric(Data$Conductivity_uS_per_cm)
      Turbidity_FNU <- as.numeric(Data$Turbidity_FNU)
      Site_number <- Data$Site_number



      Sonde = data.frame(time,Water_Temperature_degC,Conductivity_uS_per_cm,Turbidity_FNU,Site_number)
      # retain rows with data
      ind <- !is.na(Water_Temperature_degC) | !is.na(Conductivity_uS_per_cm) | !is.na(Turbidity_FNU)
      Sonde <- Sonde[ind,]

      if (length(Height_m)==length(time)) {
      Height = data.frame(time,Height_m,Site_number)
      # retain rows with data
      ind <- !is.na(Height_m)
      Height <- Height[ind,]
      }

      # Check if any new data was imported if rds file present
      if (file.exists(fileRDS[grepl('Sonde.rds',fileRDS)])) {
        So <- readRDS(fileRDS[grepl('Sonde.rds',fileRDS)])
        zlist <- list()
        z<-rbind(So,Sonde)  # combine
        zsites <- unique(z$Site_number)
        for (j in 1:length(zsites)) {
          zlist[[j]] <- z[grepl(zsites[j],z$Site_number),]
          zlist[[j]] <- zlist[[j]][!duplicated( zlist[[j]][,c('time')]),] # remove duplicate time at site
          zlist[[j]] <- zlist[[j]][order(zlist[[j]]$time),]  # order time at site
        }
        Sonde <- do.call(rbind,zlist)
        Sonde <- Sonde[order(Sonde$time),]  # order time at site
      }

      # Check if any new data was imported if rds file present
      if (length(Height_m)==length(time)) {
      if (file.exists(fileRDS[grepl('Height.rds',fileRDS)])) {
        Ho <- readRDS(fileRDS[grepl('Height.rds',fileRDS)])
        inew <- !is.element(Ho$time,Height$time)
        Output <- rbind(Height,Ho[inew,]) # combine
        Output <- Output[order(Output$time),] # order time
        Output <- Output[!duplicated(Output[,c('time')]),] # remove duplicate time
        Height <- Output
      }
        # save to RDS
        saveRDS(Height,paste0(Sonde_and_Height_folder,"/Height.rds"))
      }

      # save to RDS
      saveRDS(Sonde,paste0(Sonde_and_Height_folder,"/Sonde.rds"))

    }
    }

    }

  msg <- ".RDS FILES SAVED IN: SITE/Sonde_and_Height_data"

  return(NULL)

}
